<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Uddeshya  Upadhyay


  | Publications

</title>
<meta name="description" content="Building cool stuff.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Uddeshya</span>   Upadhyay
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">publications by categories in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/miccai21.png" />
    
    <abbr class="badge">MICCAI 2021</abbr>
    
  
  </div>

  <div id="uu_uncerguidedi2i" class="col-sm-8">
    
      <div class="title">Uncertainty-Guided Progressive GANs for Medical Image Translation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yanbei, Chen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tobias, Hepp,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sergios, Gatidis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zeynep, Akata
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>MICCAI-International Conference on Medical Image Computing and Computer Assisted Intervention</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://arxiv.org/pdf/2106.15542.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/2106.15542.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>
    Image-to-image translation plays a vital role in tackling various medical imaging tasks such as attenuation correction, motion correction, undersampled reconstruction, and denoising. Generative adversarial
    networks have been shown to achieve the state-of-the-art in generating
    high fidelity images for these tasks. However, the state-of-the-art GANbased frameworks do not estimate the uncertainty in the predictions
    made by the network that is essential for making informed medical decisions and subsequent revision by medical experts and has recently been
    shown to improve the performance and interpretability of the model. In
    this work, we propose an uncertainty-guided progressive learning scheme
    for image-to-image translation. By incorporating aleatoric uncertainty as
    attention maps for GANs trained in a progressive manner, we generate
    images of increasing fidelity progressively. We demonstrate the efficacy
    of our model on three challenging medical image translation tasks, including PET to CT translation, undersampled MRI reconstruction, and
    MRI motion artefact correction. Our model generalizes well in three different tasks and improves performance over state of the art under fullsupervision and weak-supervision with limited data. Code is released
    here: https://github.com/ExplainableML/UncerGuidedI2I
  </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/icmlw20.png" />
    
    <abbr class="badge">ICML-Workshop</abbr>
    
  
  </div>

  <div id="uu_icmlw1" class="col-sm-8">
    
      <div class="title">QUEST for MEDISYN: Quasi-norm based Uncertainty ESTimation for MEDical Image SYNthesis</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Upadhyay, Uddeshya,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sudarshan, Viswanath P,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Awate, Suyash P
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICML Workshop on Uncertainty &amp; Robustness in Deep Learning</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="http://www.gatsby.ucl.ac.uk/ balaji/udl2020/accepted-papers/UDL2020-paper-061.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/papers/ISBI_CSCC.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">uu_icmlw1</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICML-Workshop}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{QUEST for MEDISYN: Quasi-norm based Uncertainty ESTimation for MEDical Image SYNthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Upadhyay, Uddeshya and Sudarshan, Viswanath P and Awate, Suyash P}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICML Workshop on Uncertainty &amp; Robustness in Deep Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-061.pdf}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{papers/ISBI_CSCC.pdf}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{../assets/img/icmlw20.png}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/isbi20.png" />
    
    <abbr class="badge">ISBI 2020</abbr>
    
  
  </div>

  <div id="uu_compactresl" class="col-sm-8">
    
      <div class="title">Compact Representation Learning Using Class Specific Convolution Coders-Application to Medical Image Classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Biplap, Banerjee
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE International Symposium on Biomedical Imaging (ISBI)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://doi.org/10.1109/ISBI45749.2020.9098415" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/papers/ISBI_CSCC.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Medical image classification using deep learning techniques rely on highly curated datasets, which are difficult and expensive to obtain in real world due significant expertise required to annotate the dataset. We propose a novel framework called Class Specific Convolutional Coders (CSCC) to tackle the problem of learning highly discriminative, compact and non-redundant feature space from a relatively small amount of labelled images. We design separate attention-driven convolution network based feature extractors for the categories. These feature learning modules are further intuitively combined so as to make the whole image recognition system end-to-end trainable. Results on different medical image classification tasks show the advantages of our contributions, where our proposed methods outperforms the benchmark supervised deep convolutional networks (CNNs) trained from scratch.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/miccai19.png" />
    
    <abbr class="badge">MICCAI 2019</abbr>
    
  
  </div>

  <div id="uu_qegan" class="col-sm-8">
    
      <div class="title">A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suyash, Awate
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>MICCAI-International Conference on Medical Image Computing and Computer Assisted Intervention</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://doi.org/10.1007/978-3-030-32254-0_62" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/papers/MLQE.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks for image quality enhancement typically need large quantities of highly-curated training data comprising pairs of low-quality images and their corresponding high-quality images. While high-quality image acquisition is typically expensive and time-consuming, medium-quality images are faster to acquire, at lower equipment costs, and available in larger quantities. Thus, we propose a novel generative adversarial network (GAN) that can leverage training data at multiple levels of quality (e.g., high and medium quality) to improve performance while limiting costs of data curation. We apply our mixed-supervision GAN to (i) super-resolve histopathology images and (ii) enhance laparoscopy images by combining super-resolution and surgical smoke removal. Results on large clinical and pre-clinical datasets show the benefits of our mixed-supervision GAN over the state of the art.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/ijcnn2.png" />
    
    <abbr class="badge">IJCNN 2019</abbr>
    
  
  </div>

  <div id="uu_spindlemri" class="col-sm-8">
    
      <div class="title">Spinal Stenosis Detection in MRI using Modular Coordinate Convolutional Attention Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Badrinath, Singhal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Meenakshi, Singh
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE International Joint Conference on Neural Networks-IJCNN</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://doi.org/10.1109/IJCNN.2019.8852085" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Spinal stenosis is a condition in which a portion of spinal canal narrows and exerts pressure on nerves that travel through it causing pain and numbness that might require surgery. This narrowing can be caused by pathologies in bony structures (vertebrae) or soft tissue structures (intervertebral discs) that comprise the spine. Radiography, particularly Magnetic Resonance Imaging (MRI) is the modality of choice to evaluate stenosis and intervertebral disc pathology. Radiologists examine axial MRI scans at various levels along the spine to detect stenosis. Further, they evaluate the diameters of spinal canal and bulging in nearby discs which can indicate narrowing and compression on nerves. Hence measuring various diameters in a scan is a crucial step in diagnosis. However, affected regions occupy a very small fraction of the scan and there is virtually no room for error as a deviation of few pixels will also lead to discrepancies in measured and original lengths which makes it a very difficult and laborious task to measure the length of such intricate structures accurately. We propose a novel deep learning based solution to tackle this problem. Our method attempts to solve it in two independent modules and allows us to make prediction on the enlarged section of the scan which also makes it easier to measure various lengths. Human radiologists focus on certain parts of the scan rather than attending to the entire scan which largely consists of irrelevant background. We design our modular approach to mimic this attention mechanism. Both of our modules are built using coordinate convolutional networks, we also perform the comparison with baseline and empirically demonstrate superiority of our approach.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/isbi19.png" />
    
    <abbr class="badge">ISBI 2019</abbr>
    
  
  </div>

  <div id="uu_rsrgan" class="col-sm-8">
    
      <div class="title">Robust Super-Resolution GAN, with Manifold-Based and Perception Loss</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suyash, Awate
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE International Symposium on Biomedical Imaging (ISBI)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/8759375" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/papers/rsrgan.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Super-resolution using deep neural networks typically relies on highly curated training sets that are often unavailable in clinical deployment scenarios. Using loss functions that assume Gaussian-distributed residuals makes the learning sensitive to corruptions in clinical training sets. We propose novel loss functions that are robust to corruptions in training sets by modeling heavy-tailed non-Gaussian distributions on the residuals. We propose a loss based on an autoencoder-based manifold-distance between the super-resolved and high-resolution images, to reproduce realistic textural content in super-resolved images. We propose to learn to super-resolve images to match human perceptions of structure, luminance, and contrast. Results on a large clinical dataset shows the advantages of each of our contributions, where our framework improves over the state of the art.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/rbatch2.png" />
    
    <abbr class="badge">Arxiv</abbr>
    
  
  </div>

  <div id="uu_rbatch" class="col-sm-8">
    
      <div class="title">Removal of Batch Effects using GANs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Arjun, Jain
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://arxiv.org/pdf/1901.06654.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Many biological data analysis processes like Cytometry or Next Generation Sequencing (NGS) produce massive amounts of data which needs to be processed in batches for down-stream analysis. Such datasets are prone to technical variations due to difference in handling the batches possibly at different times, by different experimenters or under other different conditions. This adds variation to the batches coming from the same source sample. These variations are known as Batch Effects. It is possible that these variations and natural variations due to biology confound but such situations can be avoided by performing experiments in a carefully planned manner. Batch effects can hamper downstream analysis and may also cause results to be inconclusive. Thus, it is essential to correct for these effects. This can be solved using a novel Generative Adversarial Networks (GANs) based framework that is proposed here, advantage of using this framework over other prior approaches is that here it is not required to choose a reproducing kernel and define its parameters. Results of the framework on a mass cytometry dataset are reported.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" width="1000" src="../assets/img/tfmgames1.png" />
    
    <abbr class="badge">Arxiv</abbr>
    
  
  </div>

  <div id="uu_trl" class="col-sm-8">
    
      <div class="title">Transformer based Reinforcement Learning for Games</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Uddeshya, Upadhyay,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nikunj, Jain,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sucheta, R,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Mayanka, M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://arxiv.org/pdf/1912.03918.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent times have witnessed sharp improvements in reinforcement learning tasks using deep reinforcement learning
  techniques like Deep Q Networks, Policy Gradients, Actor
  Critic methods which are based on deep learning based models and back-propagation of gradients to train such models.
  An active area of research in reinforcement learning is about
  training agents to play complex video games, which so far
  has been something accomplished only by human intelligence. Some state of the art performances in video game
  playing using deep reinforcement learning are obtained by
  processing the sequence of frames from video games, passing them through a convolutional network to obtain features
  and then using recurrent neural networks to figure out the
  action leading to optimal rewards. The recurrent neural network will learn to extract the meaningful signal out of the
  sequence of such features. In this work, we propose a method
  utilizing transformer networks which have recently replaced
  RNNs in Natural Language Processing (NLP), and perform
  experiments to compare with existing methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Uddeshya  Upadhyay.
    
    
    
    Last updated: July 10, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
